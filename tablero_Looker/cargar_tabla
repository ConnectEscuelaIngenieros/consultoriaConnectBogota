import os
import sys
import glob
import re
from pathlib import Path
from google.cloud import bigquery

# ==============================
# Configuration (optional)
# ==============================
# If you leave these as "", the script will ask (with menus) and choose sensible defaults.
project_identifier = ""           # e.g., "my-gcp-project". If empty, the default ADC project is used.
preferred_dataset_identifier = "" # e.g., "analytics_dataset". If empty, you can pick from existing datasets.
preferred_table_identifier = ""   # e.g., "events_raw". If empty, derived from filename.

# ==============================
# Small helpers (explicit + readable)
# ==============================
def print_menu(title: str, options: list[str]) -> int:
    print(f"\n{title}")
    for index, value in enumerate(options, start=1):
        print(f"  {index}. {value}")
    while True:
        choice = input("Choose a number (default 1): ").strip()
        if choice == "":
            return 0
        if choice.isdigit() and 1 <= int(choice) <= len(options):
            return int(choice) - 1
        print("[DEBUG] Invalid choice. Try again.")

def list_local_data_files() -> list[str]:
    # Windows-friendly search for CSV and Parquet in current folder
    candidates = sorted(glob.glob("*.csv")) + sorted(glob.glob("*.parquet"))
    return candidates

def sanitize_table_name(name: str) -> str:
    # BigQuery table: letters, numbers, underscores; cannot start with number
    base = re.sub(r"[^A-Za-z0-9_]+", "_", name)
    base = re.sub(r"_+", "_", base).strip("_").lower()
    if not base:
        base = "table_from_file"
    if base[0].isdigit():
        base = f"t_{base}"
    return base[:1024]  # BigQuery limit safeguard

def infer_source_format_from_extension(file_path: str):
    suffix = Path(file_path).suffix.lower()
    if suffix == ".parquet":
        return bigquery.SourceFormat.PARQUET
    if suffix == ".csv":
        return bigquery.SourceFormat.CSV
    raise ValueError("Unsupported file type. Use .csv or .parquet.")

def choose_dataset(client: bigquery.Client, preferred: str | None) -> str:
    # If preferred is set and exists, use it; else show a menu of existing datasets.
    datasets = list(client.list_datasets())
    dataset_ids = [d.dataset_id for d in datasets]

    if preferred and preferred in dataset_ids:
        print(f"[DEBUG] Using existing dataset: {preferred}")
        return preferred

    if not dataset_ids:
        # No datasets exist; offer to create a default one
        default_name = preferred or "analytics_dataset"
        print(f"[DEBUG] No datasets found. Creating dataset: {default_name}")
        ds_ref = bigquery.Dataset(f"{client.project}.{default_name}")
        ds_ref.location = "us"  # change if you need a different location
        client.create_dataset(ds_ref, exists_ok=True)
        return default_name

    # Show menu to pick an existing dataset
    index = print_menu("Select a BigQuery dataset:", dataset_ids)
    chosen = dataset_ids[index]
    print(f"[DEBUG] Using dataset: {chosen}")
    return chosen

def choose_file_interactively() -> str:
    files = list_local_data_files()
    if not files:
        print("[DEBUG] No *.csv or *.parquet files found in the current folder.")
        print("Place your file next to this script and run again.")
        sys.exit(1)
    if len(files) == 1:
        print(f"[DEBUG] Found one file: {files[0]}")
        return files[0]
    idx = print_menu("Select a file to upload:", files)
    return files[idx]

def build_table_id(project_id: str, dataset_id: str, table_id: str) -> str:
    return f"{project_id}.{dataset_id}.{table_id}"

# ==============================
# Main upload logic
# ==============================
def main():
    # Create client
    client = bigquery.Client(project=project_identifier or None)
    effective_project = client.project
    print(f"[DEBUG] Effective project: {effective_project}")

    # Choose local file
    local_file_path = choose_file_interactively()
    print(f"[DEBUG] Selected file: {local_file_path}")

    # Determine dataset
    dataset_id = choose_dataset(client, preferred_dataset_identifier or None)

    # Determine table name
    if preferred_table_identifier:
        table_id = preferred_table_identifier
    else:
        stem = Path(local_file_path).stem
        table_id = sanitize_table_name(stem)
    print(f"[DEBUG] Target table: {table_id}")

    # Build job config
    source_format = infer_source_format_from_extension(local_file_path)
    job_config = bigquery.LoadJobConfig(
        source_format=source_format,
        autodetect=True,                 # Let BigQuery infer schema and types
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Replace table if it exists
    )

    if source_format == bigquery.SourceFormat.CSV:
        # If your CSV has a header row, set skip_leading_rows=1
        job_config.skip_leading_rows = 1
        print("[DEBUG] CSV detected. Assuming header row; set skip_leading_rows=1.")

    full_table_id = build_table_id(effective_project, dataset_id, table_id)
    print(f"[DEBUG] Loading into: {full_table_id}")

    # Upload file
    with open(local_file_path, "rb") as file_handle:
        load_job = client.load_table_from_file(
            file_handle,
            destination=full_table_id,
            job_config=job_config,
        )
    print("[DEBUG] Load job started. Waiting for completion...")
    load_job.result()  # Waits for the job to complete

    # Confirm result
    table = client.get_table(full_table_id)
    print(f"[DEBUG] Upload complete. Rows: {table.num_rows}, Columns: {len(table.schema)}")
    print(f"[DEBUG] Done: {full_table_id}")

if __name__ == "__main__":
    try:
        main()
    except Exception as error:
        print(f"[DEBUG] Error: {error}")
        sys.exit(1)
